{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import random\n",
    "from numpy.random import choice\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from scipy.signal import fftconvolve\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit, GridSearchCV\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from yield_strength_model_dual_phase_steels import *\n",
    "from PlotFunctions import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forest is built up manually by seperate trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF(n_trees):\n",
    "    \"\"\"\n",
    "    Creates Random forest model.\n",
    "    ----------\n",
    "    n_trees: integer.\n",
    "        Number of trees.\n",
    "    Returns\n",
    "    -------\n",
    "    trees: array\n",
    "        Array of tree object instances.\n",
    "    \"\"\"\n",
    "    # Creates n untrained trees to create RF model\n",
    "    ccp_value = 0\n",
    "    trees = [DecisionTreeRegressor(max_depth=10, max_features='sqrt', min_samples_split=2, min_samples_leaf=1, ccp_alpha=ccp_value, random_state=None) for i in range(n_trees)]\n",
    "    return trees\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tree is trained in a 'Random forest' fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRF(x_matrix, y_matrix, trees, Y_range=range(2,4)):\n",
    "    \"\"\"\n",
    "    Trains Random forest.\n",
    "    ----------\n",
    "    x_matrix: data points x input dim np.array\n",
    "        Input data.\n",
    "    y_matrix: data points x output dim np.array\n",
    "        Output data.\n",
    "    trees: array\n",
    "        All tree objects.\n",
    "    Y_range: range\n",
    "        Range of relevant output values.\n",
    "    Returns\n",
    "    -------\n",
    "    samples: n_trees x data points np.array\n",
    "        Matrix contains the data samples each tree has been trained on.\n",
    "    trained_trees: array\n",
    "        Array of trained tree objects\n",
    "    scalerX: scaler object\n",
    "        Scaler for input.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constants\n",
    "    n_trees = len(trees)\n",
    "    n_samples = x_matrix.shape[0]\n",
    "\n",
    "    # Create and fit scaler to training data\n",
    "    scalerX = StandardScaler(with_mean=True, with_std=True)\n",
    "    scalerX.fit(x_matrix)\n",
    "    X = scalerX.transform(x_matrix)\n",
    "    Y = y_matrix[:,Y_range]\n",
    "\n",
    "    # Bootstrap samples\n",
    "    samples = np.zeros([n_trees, n_samples])\n",
    "    for i in range(n_trees):\n",
    "        samples[i,:] = np.random.choice(n_samples, n_samples, replace=True)\n",
    "    samples = samples.astype(int)\n",
    " \n",
    "    # Train a separate tree on each bootstrapped sample\n",
    "    trained_trees = [trees[i].fit(X[samples[i,:]], Y[samples[i,:]]) for i in range(n_trees)]    \n",
    "\n",
    "    return samples, trained_trees, scalerX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(x_matrix, X_test, samples, scalerX, trees):\n",
    "    \"\"\"\n",
    "    Trains Random forest.\n",
    "    ----------\n",
    "    x_matrix: np.array\n",
    "        Input data.\n",
    "    X_test: np.array\n",
    "        Candidates to predict.\n",
    "    samples: np.array\n",
    "        Bootstrap samples.\n",
    "    scalerX: scaler object\n",
    "        Scaling object.\n",
    "    trees: object\n",
    "        Tree objects.\n",
    "    Returns\n",
    "    -------\n",
    "    norm_weights: np.array\n",
    "        Normalized weights used for possibly improved voting.\n",
    "    \"\"\"\n",
    "    n_trees = samples.shape[0]\n",
    "    n_candidates = X_test.shape[0]\n",
    "    x_matrix = scalerX.transform(x_matrix)\n",
    "    X_test = scalerX.transform(X_test)\n",
    "\n",
    "    mean_tree = np.mean(x_matrix[samples,:], axis=1)\n",
    "\n",
    "    tree_predictions, RF_prediction, mean_tree = predict(mean_tree, trees, scalerX, weights=[0])\n",
    "\n",
    "    # The distance in a TSNE projection is used to determine how close each tree is to a new candidate material.\n",
    "    tsne = TSNE(n_components=2, init='pca', learning_rate=200.0)\n",
    "    meanTsne = tsne.fit_transform(mean_tree)\n",
    "    testTsne = tsne.fit_transform(X_test)\n",
    "\n",
    "    output_size = trees[0].predict(X_test).shape[1]\n",
    "\n",
    "    weights = np.zeros([n_trees, n_candidates, output_size])\n",
    "    for i in range(n_trees):\n",
    "        weights[i,:,:] = np.tile(1/np.linalg.norm((meanTsne[i] - testTsne), axis=1), (output_size, 1)).T\n",
    "\n",
    "    norm_weights = weights/np.sum(weights, axis=0)\n",
    "\n",
    "    return norm_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the created trees and test data a prediction is made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, trees, scalerX, weights):\n",
    "    \"\"\"\n",
    "    Makes predictions based on tree objects and X_test data.\n",
    "    ----------\n",
    "    X_test: candidate data points x input dim np.array.\n",
    "        Input data for candidate materials.\n",
    "    trees: array\n",
    "        All tree objects.\n",
    "    scalerX: scaler object.\n",
    "        Scaler for input.\n",
    "    Returns\n",
    "    -------\n",
    "    tree_predictions: n_trees x n_candidates\n",
    "        Predictions for each data point made by each tree.\n",
    "    RF_prediction:\n",
    "        Prediction made by Random forest. \n",
    "    X_test: candidate data points x input dim np.array.\n",
    "        Input data for candidate materials.\n",
    "    \"\"\"\n",
    "\n",
    "    # Constants\n",
    "    n_candidates = X_test.shape[0]\n",
    "    n_trees = len(trees)\n",
    "    \n",
    "    # Transform test data using scalar object\n",
    "    X_test = scalerX.transform(X_test)\n",
    "\n",
    "    # Size of prediction\n",
    "    output_size = trees[0].predict(X_test).shape[1]\n",
    "\n",
    "    # Make predictions using each tree. 3D numpy-array on format tree_predictions[tree number, candidate point, property (yield strength, elongation)]\n",
    "    tree_predictions = np.zeros([n_trees, n_candidates, output_size])\n",
    "    for i, tree in enumerate(trees):\n",
    "        tree_predictions[i,:,:] = tree.predict(X_test)\n",
    "\n",
    "    if np.sum(weights) == 0:\n",
    "        RF_prediction = np.average(tree_predictions, axis=0)\n",
    "    else:\n",
    "        RF_prediction = np.average(tree_predictions, axis=0, weights=weights)\n",
    "\n",
    "    # Inverse transform of data\n",
    "    X_test = scalerX.inverse_transform(X_test)\n",
    "\n",
    "    return tree_predictions, RF_prediction, X_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all neccesary functions to create new forest instance and make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForestAll(n_trees, x_matrix, y_matrix, n_candidates, weighted = False, Y_range=range(2,4)):\n",
    "    \"\"\"\n",
    "    Creates RF model and produces predictions on candidate materials.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_trees: integer\n",
    "        Number of trees in model.\n",
    "    x_matrix: data points x input dim np.array\n",
    "        Input data.\n",
    "    y_matrix: data points x output dim np.array\n",
    "        Output data.\n",
    "    n_candidates: integer.\n",
    "        Number of candidate materials to be produced.\n",
    "    Y_range: range\n",
    "        Range of relevant output values.\n",
    "    Returns\n",
    "    -------\n",
    "    samples: n_trees x data points np.array\n",
    "        Matrix contains the data samples each tree has been trained on.\n",
    "    tree_predictions: n_trees x n_candidates\n",
    "        Predictions for each data point made by each tree.\n",
    "    RF_prediction:\n",
    "        Prediction made by Random forest.\n",
    "    trees: array of object instances.\n",
    "        All tree objects. \n",
    "    X_test: candidate data points x input dim np.array.\n",
    "        Input data for candidate materials.\n",
    "    Y_test: candidate data points x output dim np.array\n",
    "        Output data for candidate materials.\n",
    "    scalerX: scaler object.\n",
    "        Scaler for input.\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = np.zeros([1])\n",
    "    trees = RF(n_trees)\n",
    "    samples, trained_trees, scalerX = trainRF(x_matrix, y_matrix, trees, Y_range=range(2,4))\n",
    "    X_test, Y_test = data_generation(n = n_candidates, seed = 0)\n",
    "\n",
    "    if weighted:\n",
    "        weights = get_weights(x_matrix, X_test, samples, scalerX, trained_trees)\n",
    "    tree_predictions, RF_prediction, X_test = predict(X_test, trained_trees, scalerX, weights)\n",
    "\n",
    "    return samples, tree_predictions, RF_prediction, trained_trees, X_test, Y_test, scalerX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty measure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainty measure inspired by 'Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife' with correction of negative variance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_measure_CI(samples, tree_predictions, RF_prediction, trees, correction, dim):\n",
    "    \"\"\"\n",
    "    Calculates the variance estimates of the predictions. Base of the uncertainty measure.\n",
    "    Parameters\n",
    "    ----------\n",
    "    samples: n_trees x data points np.array\n",
    "        Matrix contains the data samples each tree has been trained on.\n",
    "    tree_predictions: n_trees x n_candidates\n",
    "        Predictions for each data point made by each tree.\n",
    "    RF_prediction:\n",
    "        Prediction made by Random forest.\n",
    "    trees: array\n",
    "        All tree objects.\n",
    "    correcction: Bool\n",
    "        Decides if correction should be used.\n",
    "    dim: integer 0/1\n",
    "        Sets which dimension of the uncertainty measure should be corrected.\n",
    "    Returns\n",
    "    -------\n",
    "    An array of variance estimates\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = samples.shape[1]\n",
    "    n_trees = tree_predictions.shape[0]\n",
    "    \n",
    "    # Create n_ji matrix showing on what each tree was trained\n",
    "    n_ji = np.zeros([n_trees, n_samples])\n",
    "    for j in range(n_trees):\n",
    "        for i, sample in enumerate(samples[j,:]):\n",
    "            n_ji[j,sample] += 1\n",
    "\n",
    "    # Expected value of n_ji\n",
    "    E_nji = 1\n",
    "       \n",
    "    # Individual tree predictions and forest prediction\n",
    "    t_jx = tree_predictions\n",
    "    E_tjx = RF_prediction\n",
    "\n",
    "    V_IJ = np.sum((np.dot(np.transpose(t_jx - E_tjx), (n_ji - E_nji))/n_trees)**2, axis=2).T\n",
    "    \n",
    "    # Bias correction\n",
    "    T = n_trees\n",
    "    n = n_samples\n",
    "    v = np.var(tree_predictions, axis=0)\n",
    "    bias_corr = (n*v/T)\n",
    "\n",
    "    # Total result\n",
    "    V_IJ_unbiased = V_IJ - bias_corr\n",
    "    V_IJ_unbiased = V_IJ_unbiased[:,dim]\n",
    "\n",
    "    \n",
    "    if correction:\n",
    "        # Calibration is a correction for converging quicker to the case of infinite n_estimators,\n",
    "        # as presented in Wager (2014) http://jmlr.org/papers/v15/wager14a.html\n",
    "        calibration_ratio = 2\n",
    "        n_sample = np.ceil(n_trees / calibration_ratio)\n",
    "\n",
    "        new_trees = [copy.deepcopy(trees[i]) for i in range(len(trees))]\n",
    "\n",
    "        random_idx = np.random.permutation(len(new_trees))[: int(n_sample)]\n",
    "        new_trees = np.array(new_trees)\n",
    "\n",
    "        new_samples = samples[random_idx]\n",
    "        new_tree_predictions = tree_predictions[random_idx]\n",
    "        new_RF_prediction = np.mean(new_tree_predictions, axis=0)\n",
    "        \n",
    "        reference_varince = uncertainty_measure_CI(new_samples, new_tree_predictions, new_RF_prediction, trees, correction = False, dim = dim)\n",
    "\n",
    "        # Use this second set of variance estimates to estimate scale of Monte Carlo noise\n",
    "        sigma2_ss = np.mean((reference_varince - V_IJ_unbiased) ** 2)\n",
    "\n",
    "        delta = n_sample / n_trees\n",
    "        sigma2 = (delta ** 2 + (1 - delta) ** 2) / (2 * (1 - delta) ** 2) * sigma2_ss\n",
    "\n",
    "        # Use Monte Carlo noise scale estimate for empirical Bayes calibration\n",
    "        V_IJ_calibrated = calibrateEB(V_IJ_unbiased, sigma2)\n",
    "\n",
    "        return V_IJ_calibrated\n",
    "    else:\n",
    "        return V_IJ_unbiased       \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start of calibration to mitigate negative uncertainty estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrateEB(variances, sigma2):\n",
    "    \"\"\"\n",
    "    Calibrate noisy variance estimates with empirical Bayes.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vars: ndarray\n",
    "        List of variance estimates.\n",
    "    sigma2: int\n",
    "        Estimate of the Monte Carlo noise in vars.\n",
    "    Returns\n",
    "    -------\n",
    "    An array of the calibrated variance estimates\n",
    "    \"\"\"\n",
    "\n",
    "    # General error check\n",
    "    if (sigma2 <= 0 or min(variances) == max(variances)):\n",
    "        return(np.maximum(variances, 0)) \n",
    "    sigma = np.sqrt(sigma2)\n",
    "    # calculated prior g\n",
    "    eb_prior = gfit(variances, sigma)\n",
    "\n",
    "    # Set up a partial execution of the function\n",
    "    part = functools.partial(gbayes, g_est=eb_prior, sigma=sigma)\n",
    "    \n",
    "    # Calculates posterior distribution given all instances of the calculated variance \n",
    "    if len(variances) >= 200:\n",
    "        # Interpolate to speed up computations:\n",
    "        calib_x = np.percentile(variances,\n",
    "                                np.arange(0, 102, 2))\n",
    "        calib_y = list(map(part, calib_x))\n",
    "        calib_all = np.interp(variances, calib_x, calib_y)\n",
    "    else:\n",
    "        calib_all = list(map(part, variances))\n",
    "\n",
    "    return np.asarray(calib_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbayes(x0, g_est, sigma):\n",
    "    \"\"\"\n",
    "    Estimate Bayes posterior with Gaussian noise [Efron2014]_.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x0: ndarray\n",
    "        an observation\n",
    "    g_est: float\n",
    "        a prior density, as returned by gfit\n",
    "    sigma: int\n",
    "        noise estimate\n",
    "    Returns\n",
    "    -------\n",
    "    An array of the posterior estimate E[mu | x0]\n",
    "    \"\"\"\n",
    "\n",
    "    Kx = norm().pdf((g_est[0] - x0) / sigma) # Likelihood Pr(data|g_est[0])\n",
    "    # Bayes theorem\n",
    "    post = Kx * g_est[1] # Likelihood * prior\n",
    "    post /= sum(post) # Divide by evidence\n",
    "    \n",
    "    return sum(post * g_est[0]) # Returns expected value E[mu|x0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gfit(X, sigma, q=5, nbin=200, unif_fraction=0.1):\n",
    "    \"\"\"\n",
    "    Fit empirical Bayes prior in the hierarchical model [Efron2014]_.\n",
    "    .. math::\n",
    "        theta ~ G, X ~ N(theta, sigma^2)\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ndarray\n",
    "        A 1D array of observations.\n",
    "    sigma: float\n",
    "        Noise estimate on X.\n",
    "    q: int\n",
    "        Number of parameters used to fit G. Default: 5\n",
    "    nbin: int\n",
    "        Number of bins used for discrete approximation.\n",
    "        Default: 200\n",
    "    unif_fraction: float\n",
    "        Fraction of G modeled as \"slab\". Default: 0.1\n",
    "    Returns\n",
    "    -------\n",
    "    An array of the posterior density estimate g.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create interval in which real variance parameter could be\n",
    "    min_x = min(min(X) - 2 * np.std(X, ddof=1), 0)\n",
    "    max_x = max(max(X) + 2 * np.std(X, ddof=1),\n",
    "                np.std(X, ddof=1))\n",
    "    xvals = np.linspace(min_x, max_x, nbin)\n",
    "    binw = (max_x - min_x) / (nbin - 1)\n",
    "\n",
    "    zero_idx = max(np.where(xvals <= 0)[0])\n",
    "\n",
    "    # Creates a noise kernel.\n",
    "    noise_kernel = norm().pdf(xvals / sigma) * binw / sigma\n",
    "\n",
    "    if zero_idx > 0:\n",
    "        noise_rotate = noise_kernel[list(np.arange(zero_idx, len(xvals))) +\n",
    "                                    list(np.arange(0, zero_idx))]\n",
    "    else:\n",
    "        noise_rotate = noise_kernel\n",
    "\n",
    "    # Creates Q matrix. See Report for more info.\n",
    "    Q = np.zeros((q, len(xvals)), dtype=\"float\")\n",
    "    for ind, exp in enumerate(range(1, q+1)):\n",
    "        mask = np.ones_like(xvals)\n",
    "        mask[np.where(xvals <= 0)[0]] = 0\n",
    "        Q[ind, :] = pow(xvals, exp) * mask\n",
    "    Q = Q.T\n",
    "\n",
    "    def neg_loglik1(alpha):\n",
    "        mask = np.ones_like(xvals) # Only positive values are allowed\n",
    "        mask[np.where(xvals <= 0)[0]] = 0\n",
    "        g_alpha_raw = np.exp(np.dot(Q, alpha) - np.max(np.dot(Q, alpha))) * mask # First part of g(alpha)\n",
    "        \n",
    "        if np.sum(g_alpha_raw) <= 100 * np.finfo(np.double).tiny: # Checks for to small values\n",
    "                return (1000 * (len(X) + sum(alpha ** 2)))\n",
    "\n",
    "        g_alpha_main = g_alpha_raw / sum(g_alpha_raw) # Last step in getting g(alpha)        \n",
    "        g_alpha = ((1 - unif_fraction) * g_alpha_main + unif_fraction * mask / sum(mask)) # Manipulates g_alpha_main in order to put a probability over all possible values of the variance parameter. Non-negative.\n",
    "        f_alpha = fftconvolve(g_alpha, noise_rotate, mode='same') # Convolution between g_alpha prior and the noise kernel(rotate). Evaluates likelihood.\n",
    "\n",
    "        return np.sum(np.interp(X, xvals, -np.log(np.maximum(f_alpha, 0.0000001)))) # Sum of the total likelihood. This is minimized. np.maximum is there to protect from negative values in np.log.\n",
    "\n",
    "    alpha_hat = minimize(neg_loglik1, list(itertools.repeat(-1, q))).x\n",
    "    \n",
    "    g_alpha_raw = np.exp(np.dot(Q, alpha_hat) - np.max(np.dot(Q, alpha_hat))) * mask\n",
    "    g_alpha_main = g_alpha_raw / sum(g_alpha_raw)\n",
    "\n",
    "    # Manipulates g_alpha_main in order to put a probability over all possible values of the variance parameter \n",
    "    g_alpha = ((1 - unif_fraction) * g_alpha_main + unif_fraction * mask) / sum(mask)\n",
    "\n",
    "    return xvals, g_alpha"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection strategy helping to evaluate results based on strategy criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection_strategy(RF_prediction, sigma_x, strategy, goal_value, closeness_crit, y_matrix, share=0):\n",
    "    \"\"\"\n",
    "    Evaluate all candidate predictions and uncertainties and choose the best one based on\n",
    "    strategy, goal_value and closeness criteria.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    RF_predictions: np.array\n",
    "        Prediction values of all candidate materials.\n",
    "    sigma_x: np.array\n",
    "        Uncertainties in predictions.\n",
    "    strategy: string.\n",
    "        Can take values 'BPV', 'OVU' or 'LU'.\n",
    "    goal_value: array\n",
    "        Array specifying where algorithm should search for new materials in the output space.\n",
    "    closeness_crit: string.\n",
    "        Can take values 'Projection' or 'Pythagoras'.\n",
    "    share: float.\n",
    "        Value between 0 and 1 indicating share of 'BPV' vs. LU strategy used.\n",
    "    Returns\n",
    "    -------\n",
    "    result_index: Integer\n",
    "        Index of best candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Normalizing in terms of maximum possible values\n",
    "    y_max_matrix = y_matrix\n",
    "\n",
    "    # Normalize candidate values in terms of maximum possible values\n",
    "    factors = np.max(y_max_matrix[:,2:4], axis=0)\n",
    "    normalized_uncertainty = sigma_x/factors\n",
    "    normalized_prediction = RF_prediction/factors\n",
    "    normalized_goal = goal_value/factors\n",
    "    # Finds angle corresponding to the goal value\n",
    "    priority_angle = np.arctan(normalized_goal[0]/normalized_goal[1])\n",
    "    result_index = None # Dummy\n",
    "\n",
    "    if strategy == 'Random':\n",
    "        strategy = np.random.choice(['BPV', 'LU'], p=[share, 1-share])\n",
    "\n",
    "    if closeness_crit == 'Projection':\n",
    "        \n",
    "        if strategy == 'BPV': # Choose highest predicted valued data point\n",
    "            projections = normalized_prediction[:,0]*np.cos(np.pi/2 - priority_angle) + normalized_prediction[:,1]*np.cos(priority_angle)\n",
    "            result_index = np.argmax(projections)\n",
    "    \n",
    "        elif strategy == 'LU': # Choose largest uncertainty data point\n",
    "            projections = normalized_uncertainty[:,0]*np.cos(np.pi/2 - priority_angle) + normalized_uncertainty[:,1]*np.cos(priority_angle)\n",
    "            result_index = np.argmax(projections)\n",
    "        \n",
    "        elif strategy == 'OVU': # Choose largest combined predicted value and uncertainty\n",
    "            projections = (normalized_prediction[:,0] + normalized_uncertainty[:,0])*np.cos(np.pi/2 - priority_angle) + (normalized_prediction[:,1] + normalized_uncertainty[:,1])*np.cos(priority_angle)\n",
    "            result_index = np.argmax(projections)\n",
    "\n",
    "        else:\n",
    "            print('ERROR: choose valid strategy')\n",
    "        \n",
    "    elif closeness_crit == 'Pythagoras':\n",
    "\n",
    "        if strategy == 'BPV': # Choose closest predicted valued data point\n",
    "            distance = np.sqrt(np.sum((normalized_prediction - normalized_goal)**2, axis=1))\n",
    "            result_index = np.argmin(distance)\n",
    "    \n",
    "        elif strategy == 'LU': # Choose largest uncertainty data point\n",
    "            abs_diff = np.abs(normalized_goal - normalized_prediction)\n",
    "            angle_to_goal = np.arctan(abs_diff[:,1]/abs_diff[:,0])\n",
    "            uncertainty = normalized_uncertainty[:,0]*np.cos(np.pi/2 - angle_to_goal) + normalized_uncertainty[:,1]*np.cos(angle_to_goal)\n",
    "            result_index = np.argmax(uncertainty)\n",
    "        \n",
    "        elif strategy == 'OVU': # Choose value closest to goal value based on predicted value and uncertainty\n",
    "            abs_diff = np.abs(normalized_goal - normalized_prediction)\n",
    "            angle_to_goal = np.arctan(abs_diff[:,1]/abs_diff[:,0])\n",
    "            projected_to_goal = normalized_uncertainty[:,0]*np.cos(np.pi/2 - angle_to_goal) + normalized_uncertainty[:,1]*np.cos(angle_to_goal)\n",
    "            E_uncertainty = projected_to_goal*np.cos(angle_to_goal)\n",
    "            YS_uncertainty = projected_to_goal*np.sin(angle_to_goal)\n",
    "            movement = np.array([YS_uncertainty, E_uncertainty]).T\n",
    "            sign_diff = ((normalized_goal - normalized_prediction)/abs_diff).astype(int)\n",
    "            resulting_coordinate = normalized_prediction + sign_diff * movement\n",
    "            distance = np.sqrt(np.sum((resulting_coordinate - normalized_goal)**2, axis=1))\n",
    "            result_index = np.argmin(distance)\n",
    "\n",
    "        else:\n",
    "            print('ERROR: choose valid strategy')\n",
    "\n",
    "    else:\n",
    "        print('ERROR: choose valid closeness criteria')\n",
    "    \n",
    "    return result_index\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculating performance based on chosen criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_performance(Y_test, index_to_add, goal_value, closeness_criteria, y_matrix):\n",
    "    \"\"\"\n",
    "    Calculate performance of found candidate material.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_test: np.array\n",
    "        Correct output values of candidate material.\n",
    "    index_to_add: Integer\n",
    "        Integer indicating which candidate material is best.\n",
    "    goal_value: array\n",
    "        Array specifying where algorithm should search for new materials in the output space.\n",
    "    closeness_crit: string.\n",
    "        Can take values 'Projection' or 'Pythagoras'.\n",
    "    y_matrix: np.array\n",
    "        Output values of all accessible data.\n",
    "    Returns\n",
    "    -------\n",
    "    performance: float\n",
    "        Performance of candidate material.\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalizing in terms of maximum possible values\n",
    "    y_max_matrix = y_matrix\n",
    "\n",
    "    # Normalize candidate values in terms of maximum possible values\n",
    "    factors = np.max(y_max_matrix[:,2:4], axis=0)\n",
    "    norm_exper_result = Y_test[index_to_add][2:4]/factors\n",
    "    #test = y_mat/factors\n",
    "    normalized_goal = goal_value/factors\n",
    "    # Finds angle corresponding to the goal value\n",
    "    priority_angle = np.arctan(normalized_goal[0]/normalized_goal[1])\n",
    "\n",
    "    # Check performance of known data\n",
    "    if closeness_criteria == 'Projection':\n",
    "        result_projection = norm_exper_result[0]*np.cos(np.pi/2 - priority_angle) + norm_exper_result[1]*np.cos(priority_angle)\n",
    "        goal_projection = normalized_goal[0]*np.cos(np.pi/2 - priority_angle) + normalized_goal[1]*np.cos(priority_angle)\n",
    "        performance = result_projection/goal_projection\n",
    "        \n",
    "    elif closeness_criteria == 'Pythagoras':\n",
    "        performance = np.sqrt(np.sum(((Y_test[index_to_add][2:4] - goal_value)/goal_value)**2))\n",
    "    \n",
    "    return performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate redsult showing bias in predictions at different stages of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_residuals(n_candidates, experiment, x_data=None, y_data=None):\n",
    "    \"\"\"\n",
    "    Calculates normalized residuals. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_candidates: Integer\n",
    "        number of candidate materials.\n",
    "    experiment: string or Bool\n",
    "        Dictates what type of data to use.\n",
    "    x_data: np.array\n",
    "        Bespoke input data.\n",
    "    y_data: np.array\n",
    "        bespoke output data.\n",
    "    Returns\n",
    "    -------\n",
    "    r_n: np.array\n",
    "        Normalized residuals.\n",
    "    sigma_x: np.array\n",
    "        Uncertainty measure.\n",
    "    \"\"\"\n",
    "\n",
    "    n_datapoints = 50\n",
    "    n_trees = int(n_datapoints/2)\n",
    "    \n",
    "    if experiment ==  True:\n",
    "        x_matrix, y_matrix = experiment_data_generation(compositions = comps, n_temp_tests = 2)\n",
    "    \n",
    "    elif experiment == 'data':\n",
    "        x_matrix, y_matrix = x_data, y_data\n",
    "    \n",
    "    else:\n",
    "        x_matrix, y_matrix = data_generation(n = n_datapoints, seed = 0)\n",
    "    \n",
    "    # Prediction with bespoke random forest model\n",
    "    samples, tree_predictions, RF_prediction, trees, X_test, Y_test, scalerX = RandomForestAll(n_trees, x_matrix, y_matrix, n_candidates, Y_range=range(2,4))\n",
    "\n",
    "    # Uncertainty in dimension (dim =) 0/1\n",
    "    sigma_x2_Y = uncertainty_measure_CI(samples, tree_predictions, RF_prediction, trees, correction = True, dim = 0)\n",
    "    sigma_x2_E = uncertainty_measure_CI(samples, tree_predictions, RF_prediction, trees, correction = True, dim = 1)\n",
    "\n",
    "    sigma_x = np.array([np.sqrt(np.abs(sigma_x2_Y)), np.sqrt(np.abs(sigma_x2_E))]).T\n",
    "\n",
    "    # normalized residuals\n",
    "    r_n = (RF_prediction - Y_test[:,2:4])/sigma_x\n",
    "\n",
    "    return r_n, sigma_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cost_diff(better_material_idx, found_x_matrix, found_y_matrix):\n",
    "    \"\"\"\n",
    "    Calculates cost difference between cheapest and most expensive composition. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    better_material_idx: np.array\n",
    "        Index of high performing materials.\n",
    "    found_x_matrix: np.array\n",
    "        High performing materials input\n",
    "    found_y_matrix: np.array\n",
    "        High performing materials output\n",
    "    Returns\n",
    "    -------\n",
    "    relative_cost_diff: float\n",
    "        Cost difference between cheap and expensive material\n",
    "    cheap_expens_chem: np.array\n",
    "        Chemistry of cheap and expensive material\n",
    "    cheap_expens_perf: np.array\n",
    "        Performance of cheap and expensive material\n",
    "    \"\"\"\n",
    "    \n",
    "    non_chem_input = 2\n",
    "    materials = found_x_matrix[better_material_idx,:]\n",
    "    performance = found_y_matrix[better_material_idx,:]\n",
    "    if materials.shape[0] == 0:\n",
    "        return 1\n",
    "    share_iron = np.array(1 - np.sum(materials[:,non_chem_input:], axis=1))\n",
    "    materials = np.hstack((materials, share_iron.reshape(-1,1)))\n",
    "    materials_cost = cost_vec()\n",
    "    cost = np.dot(materials[:,non_chem_input:], materials_cost)\n",
    "    cheap = np.min(cost)\n",
    "    expensive = np.max(cost)\n",
    "    cheap_index = np.where(cost == cheap)[0][0]\n",
    "    expensive_index = np.where(cost == expensive)[0][0]\n",
    "    relative_cost_diff = cheap/expensive\n",
    "    cheap_expens_chem = np.array([materials[cheap_index], materials[expensive_index]])\n",
    "    cheap_expens_perf = np.array([performance[cheap_index], performance[expensive_index]])\n",
    "    \n",
    "    return relative_cost_diff, cheap_expens_chem, cheap_expens_perf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluationOfAlgo(goal_value, x_matrix_stat, y_matrix_stat, n_candidates, n_experiments, strategy_list, closeness_criteria, share, weighted):\n",
    "    \"\"\"\n",
    "    Evaluates algorithm. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    goal_value: np.array\n",
    "        Goal material.\n",
    "    x_matrix_stat: np.array\n",
    "        input.\n",
    "    y_matrix_stat: np.array\n",
    "        output.\n",
    "    n_candidates: integer\n",
    "        Number of candidates to evaluate.\n",
    "    n_experiments: integer\n",
    "        Number of experiments to perform.\n",
    "    strategy_list: np.array\n",
    "        List of strategies.\n",
    "    closeness_criteria: np.array\n",
    "        How to evaluate performance.\n",
    "    share: float\n",
    "        How much BPV is used.\n",
    "    weighted: Boolean\n",
    "        Sets if weighting of RF should be used.\n",
    "    Returns\n",
    "    -------\n",
    "    n_better_vec: float\n",
    "        Number of high performing materials.\n",
    "    performance_mat: np.array\n",
    "        Performance indication for all strategies.\n",
    "    cost_diff_vec: np.array\n",
    "        Vector of cost diff.\n",
    "    cheap_expens_chem_vec: np.array\n",
    "        Chemistry of cheap and expensive material.\n",
    "    cheap_expens_perf_vec: np.array\n",
    "        Performance of cheap and expensive material.\n",
    "    \"\"\"\n",
    "\n",
    "    input_dim = x_matrix_stat.shape[1] + 1\n",
    "    output_dim = y_matrix_stat.shape[1]\n",
    "    n_better_vec = np.zeros(len(strategy_list))\n",
    "    performance_mat = np.zeros([len(strategy_list), n_experiments])\n",
    "    cost_diff_vec = np.zeros(len(strategy_list))\n",
    "    cheap_expens_chem_vec = np.zeros([len(strategy_list), 2, input_dim])\n",
    "    cheap_expens_perf_vec = np.zeros([len(strategy_list), 2, output_dim])\n",
    "    \n",
    "    # This is now the main loop for the sequential part\n",
    "    for index, strategy in enumerate(strategy_list):\n",
    "        # Generate experiment data\n",
    "        x_matrix = x_matrix_stat\n",
    "        y_matrix = y_matrix_stat\n",
    "        \n",
    "        for k in range(n_experiments):\n",
    "\n",
    "            n_datapoints = x_matrix.shape[0]\n",
    "            n_trees = int(n_datapoints/2)\n",
    "\n",
    "            samples, tree_predictions, RF_prediction, trees, X_test, Y_test, scalerX = RandomForestAll(n_trees, x_matrix, y_matrix, n_candidates, weighted = weighted[index], Y_range=range(2,4))\n",
    "\n",
    "            # Uncertainty in dimension (dim =) 0/1\n",
    "            sigma_x2_Y = uncertainty_measure_CI(samples, tree_predictions, RF_prediction, trees, correction = True, dim = 0)\n",
    "            sigma_x2_E = uncertainty_measure_CI(samples, tree_predictions, RF_prediction, trees, correction = True, dim = 1)\n",
    "            sigma_x = np.array([np.sqrt(np.abs(sigma_x2_Y)), np.sqrt(np.abs(sigma_x2_E))]).T\n",
    "            \n",
    "            index_to_add = selection_strategy(RF_prediction, sigma_x, strategy = strategy, goal_value = goal_value, closeness_crit = closeness_criteria[index], y_matrix=y_matrix, share=share[index])\n",
    "\n",
    "            x_matrix = np.concatenate((x_matrix, X_test[index_to_add,:].reshape(1,-1)), axis=0)\n",
    "            y_matrix = np.concatenate((y_matrix, Y_test[index_to_add,:].reshape(1,-1)), axis=0)\n",
    "            \n",
    "            performance = calc_performance(Y_test, index_to_add, goal_value, closeness_criteria[index], y_matrix)\n",
    "            performance_mat[index, k] = performance\n",
    "\n",
    "        \n",
    "        if closeness_criteria[index] == 'Projection':\n",
    "            n_better = np.sum((performance_mat[index,:] > 1)*1, axis=0)\n",
    "            better_material_idx = (performance_mat[index,:]>1)\n",
    "            cost_diff, cheap_expens_chem, cheap_expens_perf = calc_cost_diff(better_material_idx, x_matrix[-n_experiments:,:], y_matrix[-n_experiments:,:])\n",
    "\n",
    "        else:\n",
    "            n_better = np.sum((performance_mat[index,:] < 0.15)*1, axis=0)\n",
    "            better_material_idx = (performance_mat[index,:] < 0.15)\n",
    "            cost_diff, cheap_expens_chem, cheap_expens_perf = calc_cost_diff(better_material_idx, x_matrix[-n_experiments:,:], y_matrix[-n_experiments:,:])\n",
    "            \n",
    "        n_better_vec[index] = n_better\n",
    "        cost_diff_vec[index] = cost_diff\n",
    "        cheap_expens_chem_vec[index,:] = cheap_expens_chem\n",
    "        cheap_expens_perf_vec[index,:] = cheap_expens_perf\n",
    "\n",
    "    return n_better_vec, performance_mat, cost_diff_vec, cheap_expens_chem_vec, cheap_expens_perf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_r2(trees, scalerX, weights = np.zeros([1])):\n",
    "    \"\"\"\n",
    "    Calculates mean square error. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trees: class object\n",
    "        Tree object.\n",
    "    scalerX: class object\n",
    "        Scaler object\n",
    "    weights: np.array\n",
    "        Dictates weighting of RF.\n",
    "    Returns\n",
    "    -------\n",
    "    Mean square error\n",
    "    \n",
    "    \"\"\"\n",
    "    X_test, Y_test = data_generation(n = 1000, seed = 0)\n",
    "    tree_predictions, RF_prediction, X_test = predict(X_test, trees, scalerX, weights)\n",
    "    factors = np.max(Y_test[:,2:], axis=0)\n",
    "    \n",
    "    return np.mean(np.linalg.norm((Y_test[:,2:] - RF_prediction)/factors, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluationOfAlgo2(goal_value, x_matrix_stat, y_matrix_stat, n_candidates, n_experiments, strategy_list, closeness_criteria, share, weighted):\n",
    "    \"\"\"\n",
    "    Evaluates algorithm. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    goal_value: np.array\n",
    "        Goal material.\n",
    "    x_matrix_stat: np.array\n",
    "        input.\n",
    "    y_matrix_stat: np.array\n",
    "        output.\n",
    "    n_candidates: integer\n",
    "        Number of candidates to evaluate.\n",
    "    n_experiments: integer\n",
    "        Number of experiments to perform.\n",
    "    strategy_list: np.array\n",
    "        List of strategies.\n",
    "    closeness_criteria: np.array\n",
    "        How to evaluate performance.\n",
    "    share: float\n",
    "        How much BPV is used.\n",
    "    weighted: Boolean\n",
    "        Sets if weighting of RF should be used.\n",
    "    Returns\n",
    "    -------\n",
    "    performance_mat: np.array\n",
    "        Performance indication for all strategies.\n",
    "    \"\"\"\n",
    "    performance_mat = np.zeros([len(strategy_list), n_experiments])\n",
    "\n",
    "    # This is now the main loop for the sequential part\n",
    "    for index, strategy in enumerate(strategy_list):\n",
    "    \n",
    "        # Generate experiment data\n",
    "        x_matrix = x_matrix_stat\n",
    "        y_matrix = y_matrix_stat\n",
    "        \n",
    "        for k in range(n_experiments):\n",
    "\n",
    "            n_datapoints = x_matrix.shape[0]\n",
    "            n_trees = int(n_datapoints/2)\n",
    "\n",
    "            samples, tree_predictions, RF_prediction, trees, X_test, Y_test, scalerX = RandomForestAll(n_trees, x_matrix, y_matrix, n_candidates, weighted = weighted[index], Y_range=range(2,4))\n",
    "\n",
    "            # Uncertainty in dimension (dim =) 0/1\n",
    "            sigma_x2_Y = uncertainty_measure_CI(samples, tree_predictions, RF_prediction, trees, correction = True, dim = 0)\n",
    "            sigma_x2_E = uncertainty_measure_CI(samples, tree_predictions, RF_prediction, trees, correction = True, dim = 1)\n",
    "\n",
    "            sigma_x = np.array([np.sqrt(np.abs(sigma_x2_Y)), np.sqrt(np.abs(sigma_x2_E))]).T\n",
    "\n",
    "            if strategy == 'RS':\n",
    "                x_data, y_data = data_generation(n = 1, seed = 0)\n",
    "                x_matrix = np.concatenate((x_matrix, x_data.reshape(1,-1)), axis=0)\n",
    "                y_matrix = np.concatenate((y_matrix, y_data.reshape(1,-1)), axis=0)\n",
    "    \n",
    "            else:\n",
    "                index_to_add = selection_strategy(RF_prediction, sigma_x, strategy = strategy, goal_value = goal_value, closeness_crit = closeness_criteria[index], y_matrix=y_matrix, share=share[index])\n",
    "                x_matrix = np.concatenate((x_matrix, X_test[index_to_add,:].reshape(1,-1)), axis=0)\n",
    "                y_matrix = np.concatenate((y_matrix, Y_test[index_to_add,:].reshape(1,-1)), axis=0)\n",
    "            \n",
    "            performance = calc_r2(trees, scalerX)\n",
    "            performance_mat[index, k] = performance            \n",
    "\n",
    "    return performance_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize the forest model\n",
    "n_datapoints = 500\n",
    "x_matrix, y_matrix = data_generation(n = n_datapoints, seed = 0)\n",
    "\n",
    "Y_range = range(2,4)\n",
    "init_data = 24\n",
    "\n",
    "scalerX = StandardScaler(with_mean=True, with_std=True)\n",
    "scalerX.fit(x_matrix)\n",
    "\n",
    "X = scalerX.transform(x_matrix)\n",
    "Y = y_matrix[:,Y_range]\n",
    "\n",
    "# Initialize Leave-p-Out cross-validation\n",
    "ss = ShuffleSplit(n_splits=30, test_size=(n_datapoints-init_data)/n_datapoints , random_state=0)\n",
    "\n",
    "# Random forest model\n",
    "RFM = RandomForestRegressor()\n",
    "parameters_grid = {'max_depth': [1, 4, 7, None], 'n_estimators': [init_data*2, int(init_data/2), init_data], 'min_samples_leaf': [1], 'min_samples_split': [2], 'ccp_alpha': [0, 0.1]}\n",
    "\n",
    "RF_gs = GridSearchCV(estimator=RFM, cv=ss, param_grid=parameters_grid, verbose=1)\n",
    "gs_RF_result = RF_gs.fit(X, Y)\n",
    "\n",
    "print('Best accuracy', gs_RF_result.best_score_)\n",
    "print('Best parameteres', gs_RF_result.best_params_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST\n",
    "Test of calibration of uncertainty measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test and visualization of calibrated variance estimates\n",
    "\n",
    "comps = fetch_comp_data(filename = 'realistic_compositions_in_weight_percent.xlsx')\n",
    "\n",
    "# Generate experiment data\n",
    "x_matrix, y_matrix = experiment_data_generation(compositions = comps, n_temp_tests = 2)\n",
    "\n",
    "# Prediction with bespoke random forest model\n",
    "n_trees = 15\n",
    "n_candidates = 1000\n",
    "trees = RF(n_trees)\n",
    "weighted = False\n",
    "samples, tree_predictions, RF_prediction, trees, X_test, Y_test, scalerX = RandomForestAll(n_trees, x_matrix, y_matrix, n_candidates, weighted, Y_range=range(2,4))\n",
    "\n",
    "# Uncertainty in dimension (dim =) 0/1\n",
    "sigma_x2_Y = uncertainty_measure_CI(samples, tree_predictions, RF_prediction, trees, correction = True, dim = 0)\n",
    "sigma_x2_Y_u = uncertainty_measure_CI(samples, tree_predictions, RF_prediction, trees, correction = False, dim = 0)\n",
    "\n",
    "plt.figure(3)\n",
    "plt.hist(sigma_x2_Y, label='Empirical Bayes corrected', alpha=0.6)\n",
    "plt.hist(sigma_x2_Y_u, label='Non-corrected', alpha=0.6)\n",
    "plt.xlabel('Variance estimates', fontsize = 15)\n",
    "plt.title('Corrected and non-corrected variance estimates', fontsize=15)\n",
    "plt.legend()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to see if normalized residuals differ depending on training stratety and initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set goal as [Yield strength, Elongation]\n",
    "goal_value = np.array([1300, 7])\n",
    "\n",
    "# Fetch realistic data\n",
    "comps = fetch_comp_data(filename = 'realistic_compositions_in_weight_percent.xlsx')\n",
    "#comps = fetch_comp_data(filename = 'cluster_compositions.xlsx')\n",
    "\n",
    "# Generate experiment data\n",
    "x_matrix_stat, y_matrix_stat = experiment_data_generation(compositions = comps, n_temp_tests = 2)\n",
    "\n",
    "# Prediction with bespoke random forest model\n",
    "n_candidates = 1000\n",
    "\n",
    "# Set parameters\n",
    "weighted = [False, False, False]\n",
    "strategy_list = ['BPV', 'OVU', 'LU']\n",
    "closeness_criteria = ['Projection', 'Projection', 'Projection']#['Pythagoras']\n",
    "n_better_vec = np.zeros(len(strategy_list))\n",
    "experiment_list = np.zeros(len(strategy_list))\n",
    "\n",
    "# This is now the main loop for the sequential part\n",
    "for index, strategy in enumerate(strategy_list):\n",
    "    \n",
    "    # Generate experiment data\n",
    "    x_matrix = x_matrix_stat\n",
    "    y_matrix = y_matrix_stat\n",
    "    \n",
    "    n_experiments = 10\n",
    "    performance_list = np.zeros(n_experiments)\n",
    "\n",
    "    for k in tqdm(range(n_experiments)):\n",
    "\n",
    "        n_datapoints = x_matrix.shape[0]\n",
    "        n_trees = int(n_datapoints/2)\n",
    "\n",
    "        samples, tree_predictions, RF_prediction, trees, X_test, Y_test, scalerX = RandomForestAll(n_trees, x_matrix, y_matrix, n_candidates, weighted = weighted[index], Y_range=range(2,4))\n",
    "\n",
    "        # Uncertainty in dimension (dim =) 0/1\n",
    "        sigma_x2_Y = uncertainty_measure_CI(samples, tree_predictions, RF_prediction, trees, correction = True, dim = 0)\n",
    "        sigma_x2_E = uncertainty_measure_CI(samples, tree_predictions, RF_prediction, trees, correction = True, dim = 1)\n",
    "        sigma_x = np.array([np.sqrt(np.abs(sigma_x2_Y)), np.sqrt(np.abs(sigma_x2_E))]).T\n",
    "\n",
    "        index_to_add = selection_strategy(RF_prediction, sigma_x, strategy = strategy, goal_value = goal_value, y_matrix=y_matrix, closeness_crit = closeness_criteria[index])\n",
    "\n",
    "        x_matrix = np.concatenate((x_matrix, X_test[index_to_add,:].reshape(1,-1)), axis=0)\n",
    "        y_matrix = np.concatenate((y_matrix, Y_test[index_to_add,:].reshape(1,-1)), axis=0)\n",
    "    \n",
    "\n",
    "    plot_traversion(x_matrix, y_matrix, n_experiments, strategy, index, goal_value)\n",
    "\n",
    "    # Make histogram\n",
    "    iter = 2\n",
    "    n_cand = 5000\n",
    "    experiment = 'data'\n",
    "    r_n_vec = np.zeros([iter*n_cand, 2])\n",
    "    sigma_x_vec = np.zeros([iter*n_cand, 2])\n",
    "\n",
    "    for i in tqdm(range(iter)):\n",
    "        r_n, sigma_x = get_norm_residuals(n_cand, experiment, x_matrix, y_matrix)#x_data, y_data)\n",
    "        r_n_vec[i*n_cand:i*n_cand+n_cand,:] = r_n\n",
    "        sigma_x_vec[i*n_cand:i*n_cand+n_cand,:] = sigma_x\n",
    "\n",
    "    print('Mean: ', np.mean(r_n_vec, axis=0))\n",
    "\n",
    "    plt.figure(index + 50)\n",
    "    plt.hist(r_n_vec[:,0], bins=200, color='royalblue', lw=0, density=True, alpha=0.6, label='Yield strength, $\\sigma_y$')\n",
    "    plt.hist(r_n_vec[:,1], bins=200, color='orangered', lw=0, density=True, alpha=0.5, label='Elongation, $\\epsilon_t$')\n",
    "\n",
    "    mu = 0\n",
    "    std = 1\n",
    "    x = np.linspace(-10, 10, 100)\n",
    "    y = norm.pdf(x, mu, std)\n",
    "    plt.plot(x, y, '-k', label='N(0, 1)')\n",
    "    plt.xlim(-10,10)\n",
    "    plt.legend()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST\n",
    "\n",
    "Find examples of cheap and expensive materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set goal as [Yield strength, Elongation]\n",
    "goal_value = np.array([1300, 7])\n",
    "\n",
    "# Fetch realistic data\n",
    "comps = fetch_comp_data(filename = 'realistic_compositions_in_weight_percent.xlsx')\n",
    "# Generate experiment data with less information about input-output\n",
    "#comps = fetch_comp_data(filename = 'cluster_compositions.xlsx')\n",
    "\n",
    "print(cost_vec())\n",
    "\n",
    "# Generate experiment data\n",
    "x_matrix_stat, y_matrix_stat = experiment_data_generation(compositions = comps, n_temp_tests = 2)\n",
    "\n",
    "# Prediction with bespoke random forest model\n",
    "n_experiments = 10\n",
    "n_candidates = 1000\n",
    "\n",
    "# Set parameters\n",
    "weighted = [False]\n",
    "share = [0]\n",
    "strategy_list = ['BPV']\n",
    "closeness_criteria = ['Projection']\n",
    "\n",
    "iter = 1\n",
    "n_better_vec_tot = np.zeros(len(strategy_list))\n",
    "performance_mat_tot = np.zeros([len(strategy_list), n_experiments])\n",
    "cost_diff_vec_tot = np.zeros(len(strategy_list))\n",
    "\n",
    "print('Goal: ', goal_value)\n",
    "\n",
    "for i in tqdm(range(iter)):\n",
    "    n_better_vec, performance_mat, cost_diff_vec, cheap_expens_chem_vec, cheap_expens_perf_vec = evaluationOfAlgo(goal_value, x_matrix_stat, y_matrix_stat, n_candidates, n_experiments, strategy_list, closeness_criteria, share, weighted)\n",
    "    n_better_vec_tot += n_better_vec\n",
    "    performance_mat_tot += performance_mat\n",
    "    cost_diff_vec_tot += cost_diff_vec\n",
    "\n",
    "n_better_mean = n_better_vec_tot/iter\n",
    "performance_mat_mean = performance_mat_tot/iter\n",
    "cost_diff_vec_mean = cost_diff_vec_tot/iter\n",
    "\n",
    "print('cheap vs expensive', cheap_expens_chem_vec)\n",
    "print('cheap vs expensive perf', cheap_expens_perf_vec)\n",
    "\n",
    "print('Mean better materials: ', n_better_mean)\n",
    "print('Mean cost span: ', cost_diff_vec_mean)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    " \n",
    "Can we see what tendencies the model has picked up on? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set goal as [Yield strength, Elongation]\n",
    "goal_value = np.array([1100, 12.5])\n",
    "# [Annealing_temperature, Tempering_temperature, wC, wMn, wCr, wMo, wNi, wSi]\n",
    "test_material_annealing = np.array([0, 150, 0.2E-2, 1.5E-2, 0.5E-2, 0.5E-2, 0.5E-2, 1E-2])\n",
    "test_material_solidsol = np.array([850, 150, 0.1E-2, 1.5E-2, 0.5E-2, 0, 0.5E-2, 1E-2])\n",
    "\n",
    "eval_points = 100\n",
    "\n",
    "temps = np.linspace(700,900, eval_points)\n",
    "Mo_shares = np.linspace(0, 0.5E-2, eval_points)\n",
    "sigma_real_a = np.zeros(eval_points)\n",
    "sigma_real_s = np.zeros(eval_points)\n",
    "\n",
    "X_eval_annealing = np.zeros([eval_points, len(test_material_annealing)])\n",
    "X_eval_solidsol = np.zeros([eval_points, len(test_material_annealing)])\n",
    "\n",
    "for i in range(eval_points):\n",
    "    test_material_annealing[0] = temps[i]\n",
    "    test_material_solidsol[5] = Mo_shares[i]\n",
    "\n",
    "    X_eval_annealing[i,:] = test_material_annealing\n",
    "    X_eval_solidsol[i,:] = test_material_solidsol\n",
    "\n",
    "    y_real_a = fetch_data_scenario(test_material_annealing)\n",
    "    y_real_s = fetch_data_scenario(test_material_solidsol)\n",
    "\n",
    "    sigma_real_a[i] = y_real_a[2]\n",
    "    sigma_real_s[i] = y_real_s[2]\n",
    "\n",
    "# Fetch realistic data\n",
    "comps = fetch_comp_data(filename = 'realistic_compositions_in_weight_percent.xlsx')\n",
    "\n",
    "# Generate experiment data\n",
    "x_matrix_stat, y_matrix_stat = experiment_data_generation(compositions = comps, n_temp_tests = 2)\n",
    "#x_matrix_stat, y_matrix_stat = data_generation(n = 1000, seed = 0)\n",
    "\n",
    "# Prediction with random forest model\n",
    "n_candidates = 1000\n",
    "n_experiments = 11\n",
    "\n",
    "# Set parameters\n",
    "weighted = [False, False, False]\n",
    "strategy_list = ['BPV', 'OVU', 'LU']\n",
    "closeness_criteria = ['Pythagoras', 'Pythagoras', 'Pythagoras']\n",
    "n_better_vec = np.zeros(len(strategy_list))\n",
    "experiment_list = np.zeros(len(strategy_list))\n",
    "RF_prediction_a = np.zeros([len(strategy_list), eval_points])\n",
    "RF_prediction_s = np.zeros([len(strategy_list), eval_points])\n",
    "\n",
    "# This is now the main loop for the sequential part\n",
    "for index, strategy in enumerate(strategy_list):\n",
    "    \n",
    "    # Generate experiment data\n",
    "    x_matrix = x_matrix_stat\n",
    "    y_matrix = y_matrix_stat\n",
    "\n",
    "    for k in tqdm(range(n_experiments)):\n",
    "\n",
    "        n_datapoints = x_matrix.shape[0]\n",
    "        n_trees = int(n_datapoints/2)\n",
    "\n",
    "        samples, tree_predictions, RF_prediction, trees, X_test, Y_test, scalerX = RandomForestAll(n_trees, x_matrix, y_matrix, n_candidates, weighted=weighted[index], Y_range=range(2,4))\n",
    "\n",
    "        # Uncertainty in dimension (dim =) 0/1\n",
    "        sigma_x2_Y = uncertainty_measure_CI(samples, tree_predictions, RF_prediction, trees, correction = True, dim = 0)\n",
    "        sigma_x2_E = uncertainty_measure_CI(samples, tree_predictions, RF_prediction, trees, correction = True, dim = 1)\n",
    "        sigma_x = np.array([np.sqrt(np.abs(sigma_x2_Y)), np.sqrt(np.abs(sigma_x2_E))]).T\n",
    "\n",
    "        index_to_add = selection_strategy(RF_prediction, sigma_x, strategy = strategy, goal_value = goal_value, y_matrix=y_matrix, closeness_crit = closeness_criteria[index])\n",
    "\n",
    "        x_matrix = np.concatenate((x_matrix, X_test[index_to_add,:].reshape(1,-1)), axis=0)\n",
    "        y_matrix = np.concatenate((y_matrix, Y_test[index_to_add,:].reshape(1,-1)), axis=0)\n",
    "    \n",
    "    weights = np.zeros([1])\n",
    "    if weighted[index]:\n",
    "        weights = get_weights(x_matrix, X_test, samples, scalerX, trees)\n",
    "    tree_predictions, RF_prediction_apre, X_test = predict(X_eval_annealing, trees, scalerX, weights)\n",
    "    tree_predictions, RF_prediction_spre, X_test = predict(X_eval_solidsol, trees, scalerX, weights)\n",
    "    RF_prediction_a[index,:] = RF_prediction_apre[:,0]\n",
    "    RF_prediction_s[index,:] = RF_prediction_spre[:,0]\n",
    "\n",
    "colors = ['tab:blue', 'tab:orange', 'tab:green']\n",
    "\n",
    "plt.figure(1)\n",
    "for i in range(len(strategy_list)):\n",
    "    plt.plot(temps, RF_prediction_a[i,:], label=strategy_list[i], color=colors[i])#strategy_list[i]\n",
    "plt.plot(temps, sigma_real_a, label='True model', color='dimgrey', alpha=0.7)\n",
    "plt.xlabel('Annealing temperature [$^\\circ$C]', fontsize=14)\n",
    "plt.ylabel('$\\sigma_y$, yield strength [MPa]', fontsize=14)\n",
    "plt.legend()\n",
    "plt.title('Annealing effect', fontsize=14)\n",
    "\n",
    "plt.figure(2)\n",
    "for i in range(len(strategy_list)):\n",
    "    plt.plot(Mo_shares, RF_prediction_s[i,:], label=strategy_list[i], color=colors[i])#strategy_list[i]\n",
    "plt.plot(Mo_shares, sigma_real_s, label='True model', color = 'dimgrey', alpha=0.7)\n",
    "plt.xlabel('Molybdenum [$\\%$]', fontsize=14)\n",
    "plt.ylabel('$\\sigma_y$, yield strength [MPa]', fontsize=14)\n",
    "plt.legend()\n",
    "plt.title('Molybdenum effect', fontsize=14)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "How does MSE evolve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set goal as [Yield strength, Elongation]\n",
    "goal_value = np.array([1100, 12.5])\n",
    "\n",
    "# Fetch realistic data\n",
    "comps = fetch_comp_data(filename = 'realistic_compositions_in_weight_percent.xlsx')\n",
    "# Generate experiment data with less information about input-output\n",
    "#comps = fetch_comp_data(filename = 'cluster_compositions.xlsx')\n",
    "\n",
    "# Generate experiment data\n",
    "x_matrix_stat, y_matrix_stat = experiment_data_generation(compositions = comps, n_temp_tests = 2)\n",
    "\n",
    "# Prediction with bespoke random forest model\n",
    "n_experiments = 10\n",
    "n_candidates = 1000\n",
    "\n",
    "# Set parameters\n",
    "weighted = [False, False, False, False]\n",
    "share = [0,0,0,0]\n",
    "strategy_list = ['BPV', 'OVU', 'LU', 'RS']\n",
    "closeness_criteria = ['Pythagoras', 'Pythagoras', 'Pythagoras', 'Pythagoras']\n",
    "iter = 10\n",
    "\n",
    "performance_mat_tot = np.zeros([len(strategy_list), n_experiments])\n",
    "\n",
    "print('Goal: ', goal_value)\n",
    "\n",
    "for i in tqdm(range(iter)):\n",
    "    performance_mat = evaluationOfAlgo2(goal_value, x_matrix_stat, y_matrix_stat, n_candidates, n_experiments, strategy_list, closeness_criteria, share, weighted)\n",
    "    performance_mat_tot += performance_mat\n",
    "\n",
    "performance_mat_mean = performance_mat_tot/iter\n",
    "\n",
    "for j in range(len(strategy_list)):\n",
    "    plt.plot(performance_mat_mean[j,:], label=strategy_list[j])#label=f'{int(share[j]*100)} % BPV')\n",
    "\n",
    "plt.xlabel('Experiment')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.title('MSE')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Heatmap of uncertainties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch realistic data\n",
    "#comps = fetch_comp_data(filename = 'realistic_compositions_in_weight_percent.xlsx')\n",
    "comps = fetch_comp_data(filename = 'cluster_compositions.xlsx')\n",
    "\n",
    "# Generate experiment data\n",
    "x_matrix_stat, y_matrix_stat = experiment_data_generation(compositions = comps, n_temp_tests = 2)\n",
    "\n",
    "# Prediction with bespoke random forest model\n",
    "n_candidates = 500\n",
    "\n",
    "# Set parameters\n",
    "weighted = [False, False, False]\n",
    "strategy_list = ['LU']\n",
    "closeness_criteria = ['Projection']\n",
    "n_better_vec = np.zeros(len(strategy_list))\n",
    "experiment_list = np.zeros(len(strategy_list))\n",
    "n_experiments = 1\n",
    "performance_list = np.zeros(n_experiments)\n",
    "\n",
    "# This is now the main loop for the sequential part\n",
    "for index, strategy in enumerate(strategy_list):\n",
    "    \n",
    "    for k in tqdm(range(n_experiments)):\n",
    "\n",
    "        n_datapoints = x_matrix_stat.shape[0]\n",
    "        n_trees = int(n_datapoints/2)\n",
    "\n",
    "        samples, tree_predictions, RF_prediction, trees, X_test, Y_test, scalerX = RandomForestAll(n_trees, x_matrix_stat, y_matrix_stat, n_candidates, weighted = weighted[index], Y_range=range(2,4))\n",
    "\n",
    "        # Uncertainty in dimension (dim =) 0/1\n",
    "        sigma_x2_Y = uncertainty_measure_CI(samples, tree_predictions, RF_prediction, trees, correction = True, dim = 0)\n",
    "        sigma_x2_E = uncertainty_measure_CI(samples, tree_predictions, RF_prediction, trees, correction = True, dim = 1)\n",
    "\n",
    "        sigma_x = np.array([np.sqrt(np.abs(sigma_x2_Y)), np.sqrt(np.abs(sigma_x2_E))]).T\n",
    "\n",
    "values = np.linalg.norm(sigma_x/np.array([1400, 22]), axis=1)  # The values used for coloring\n",
    "\n",
    "# Create a colormap\n",
    "colormap = plt.cm.get_cmap('cool')\n",
    "\n",
    "plt.figure(figsize=[10, 7])\n",
    "\n",
    "# Plot the scatter plot with colored points\n",
    "plt.scatter(y_matrix_stat[:, 3], y_matrix_stat[:, 2], color='black')\n",
    "scatter = plt.scatter(Y_test[:, 3], Y_test[:, 2], c=values, cmap=colormap, vmin=0, vmax=.3)\n",
    "#scatter = plt.scatter(RF_prediction[:,1], RF_prediction[:,0], c=values, cmap=colormap, vmin=0, vmax=.3)\n",
    "\n",
    "# Add a colorbar\n",
    "colorbar = plt.colorbar(scatter)\n",
    "colorbar.set_label('Values')\n",
    "\n",
    "plt.xlim(0, 23)\n",
    "plt.ylim(0, 1400)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db11882b94d98d73e1b902c687d8f050b6cd8fa7e740895b94b4abc9222c3fff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
